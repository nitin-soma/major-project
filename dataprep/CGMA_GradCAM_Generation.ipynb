{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fa5568f",
   "metadata": {},
   "source": [
    "# CGMA and GradCAM Generation Notebook\n",
    "This notebook generates CGMA (Cumulative GradCAM Maps) and GradCAMs for your dataset using a trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df608c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow opencv-python matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35d895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5727ca",
   "metadata": {},
   "source": [
    "## Define GradCAM and CGMA Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263f2bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradCam(model, img_array, layer_name, eps=1e-8):\n",
    "    gradModel = tf.keras.models.Model(inputs=[model.inputs], outputs=[model.get_layer(layer_name).output, model.output])\n",
    "    with tf.GradientTape() as tape:\n",
    "        inputs = tf.cast(img_array, tf.float32)\n",
    "        (convOutputs, predictions) = gradModel(inputs)\n",
    "        loss = predictions[:, 0]\n",
    "    grads = tape.gradient(loss, convOutputs)\n",
    "    castConvOutputs = tf.cast(convOutputs > 0, 'float32')\n",
    "    castGrads = tf.cast(grads > 0, 'float32')\n",
    "    guidedGrads = castConvOutputs * castGrads * grads\n",
    "    convOutputs = convOutputs[0]\n",
    "    guidedGrads = guidedGrads[0]\n",
    "    weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n",
    "    cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n",
    "    (w, h) = (img_array.shape[2], img_array.shape[1])\n",
    "    heatmap = cv2.resize(cam.numpy(), (w, h))\n",
    "    numer = heatmap - np.min(heatmap)\n",
    "    denom = (heatmap.max() - heatmap.min()) + eps\n",
    "    heatmap = numer / denom\n",
    "    return heatmap\n",
    "\n",
    "def fuse_layers(layers, model, img):\n",
    "    cams = []\n",
    "    for layer in layers:\n",
    "        cam = GradCam(model, np.expand_dims(img, axis=0), layer)\n",
    "        cam = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
    "        cams.append(cam)\n",
    "    fused = np.mean(cams, axis=0)\n",
    "    return fused, cams\n",
    "\n",
    "def gradcam_ll(layers, model, img):\n",
    "    layer = layers[-1]\n",
    "    cam = GradCam(model, np.expand_dims(img, axis=0), layer)\n",
    "    cam = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
    "    return cam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030bef9f",
   "metadata": {},
   "source": [
    "## Set Paths and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5f962",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '../models/Food/VGG/Epoch=04- Loss=0.18 - val_acc = 0.96.h5'\n",
    "DATASET_PATH = '../Datasets/Food'\n",
    "# List all image paths\n",
    "image_paths = [os.path.join(DATASET_PATH, cls, fname) for cls in os.listdir(DATASET_PATH) for fname in os.listdir(os.path.join(DATASET_PATH, cls)) if fname.lower().endswith(('.jpg', '.png'))]\n",
    "labels = [cls for cls in os.listdir(DATASET_PATH) for fname in os.listdir(os.path.join(DATASET_PATH, cls)) if fname.lower().endswith(('.jpg', '.png'))]\n",
    "train_a, test_a, train_y, test_y = train_test_split(np.array(image_paths), labels, test_size=0.1, random_state=42, shuffle=False)\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(np.array(train_y).reshape(-1,1))\n",
    "train_y = enc.transform(np.array(train_y).reshape(-1,1)).toarray()\n",
    "test_y = enc.transform(np.array(test_y).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4626e025",
   "metadata": {},
   "source": [
    "## Load Model and Get Conv Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ef1e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "model.layers[-1].activation = tf.keras.activations.linear\n",
    "conv2D_layers = [layer.name for layer in reversed(model.layers) if len(layer.output_shape) == 4 and isinstance(layer, tf.keras.layers.Conv2D)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec17d28",
   "metadata": {},
   "source": [
    "## Generate and Save CGMA and GradCAMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30080ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../CGMS/Food/VGG', exist_ok=True)\n",
    "os.makedirs('../Gradcams/Food/VGG', exist_ok=True)\n",
    "o = len(train_a)\n",
    "for i in range(o):\n",
    "    os.makedirs(f'../CGMS/Food/VGG/{i}', exist_ok=True)\n",
    "    img = cv2.imread(train_a[i])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "    sp, cams = fuse_layers(conv2D_layers, model, img)\n",
    "    for j, cam in enumerate(cams):\n",
    "        plt.imsave(f'../CGMS/Food/VGG/{i}/{j}.jpg', cam, cmap='jet')\n",
    "    print(f'CGMA {i+1}/{o}', end='\n",
    "')\n",
    "for i in range(o):\n",
    "    img = cv2.imread(train_a[i])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "    cam = gradcam_ll(conv2D_layers, model, img)\n",
    "    plt.imsave(f'../Gradcams/Food/VGG/{i}.jpg', cam, cmap='jet')\n",
    "    print(f'GradCAM {i+1}/{o}', end='\n",
    "')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
